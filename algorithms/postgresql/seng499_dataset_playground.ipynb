{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install Dependencies"
      ],
      "metadata": {
        "id": "wrqHqPMW7ZO8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kY125xr1dEPL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d07d4fa-3574-4ab6-f4e5-8183554e1d1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install datasets\n",
        "!pip -q install unidecode"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "from unidecode import unidecode\n",
        "import csv\n",
        "import datetime\n",
        "\n",
        "datasets.logging.set_verbosity_error()"
      ],
      "metadata": {
        "id": "qcEU75_Gw2cX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple POC"
      ],
      "metadata": {
        "id": "U4uGE5uE7cEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.load_dataset(\n",
        "    \"McAuley-Lab/Amazon-Reviews-2023\",\n",
        "    f\"raw_meta_Electronics\",\n",
        "    split=\"full\",\n",
        "    trust_remote_code=True,\n",
        "    streaming=True\n",
        ")\n",
        "\n",
        "# dataset = dataset.shuffle(seed=42, buffer_size=1_000)\n",
        "dataset = dataset.take(10)\n",
        "\n",
        "for i, item in enumerate(dataset):\n",
        "    print(item)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gORH5EbQ99hN",
        "outputId": "eccbd9a0-9cdf-4363-b620-bdcbc0f387eb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'main_category': 'All Electronics', 'title': 'FS-1051 FATSHARK TELEPORTER V3 HEADSET', 'average_rating': 3.5, 'rating_number': 6, 'features': [], 'description': ['Teleporter V3 The “Teleporter V3” kit sets a new level of value in the FPV world with Fat Shark renowned performance and quality. The fun of FPV is experienced firsthand through the large screen FPV headset with integrated NexwaveRF receiver technology while simultaneously recording onboard HD footage with the included “PilotHD” camera. The “Teleporter V3” kit comes complete with everything you need to step into the cockpit of your FPV vehicle. We’ve included our powerful 250mW 5.8Ghz transmitter, 25 degree FOV headset (largest QVGA display available), the brand new “PilotHD” camera with live AV out and all the cables, antennas and connectors needed.'], 'price': 'None', 'images': {'hi_res': [None], 'large': ['https://m.media-amazon.com/images/I/41qrX56lsYL._AC_.jpg'], 'thumb': ['https://m.media-amazon.com/images/I/41qrX56lsYL._AC_US40_.jpg'], 'variant': ['MAIN']}, 'videos': {'title': [], 'url': [], 'user_id': []}, 'store': 'Fat Shark', 'categories': ['Electronics', 'Television & Video', 'Video Glasses'], 'details': '{\"Date First Available\": \"August 2, 2014\", \"Manufacturer\": \"Fatshark\"}', 'parent_asin': 'B00MCW7G9M', 'bought_together': None, 'subtitle': None, 'author': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from random import randrange\n",
        "\n",
        "print(randrange(4000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwxI354m7tbg",
        "outputId": "81080827-93c1-4518-f449-812c9da749d4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1814\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write `init.sql` - don't use this"
      ],
      "metadata": {
        "id": "2HbFkjm9sFHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of records per category.\n",
        "NUM_RECORDS = 10_000\n",
        "\n",
        "# Constants\n",
        "SQL_INSERT = \"INSERT INTO public.\\\"Listings\\\" (seller_id, title, price, location, postal_code, status, category) VALUES\\n\"\n",
        "LOCATION = \"'POINT(48.378400 -123.415600)'::GEOMETRY\"\n",
        "POSTAL_CODE = \"'V8R6N2'\"\n",
        "STATUS = \"'AVAILABLE'\"\n",
        "\n",
        "# from https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023/tree/main/raw/meta_categories\n",
        "categories = [\n",
        "    'Appliances',\n",
        "    'Automotive',\n",
        "    'Beauty_and_Personal_Care',\n",
        "    'Cell_Phones_and_Accessories',\n",
        "    'Clothing_Shoes_and_Jewelry',\n",
        "    'Electronics',\n",
        "    'Health_and_Household',\n",
        "    'Home_and_Kitchen',\n",
        "    'Industrial_and_Scientific',\n",
        "    'Musical_Instruments',\n",
        "    'Office_Products',\n",
        "    'Patio_Lawn_and_Garden',\n",
        "    'Sports_and_Outdoors',\n",
        "    'Tools_and_Home_Improvement',\n",
        "    'Video_Games'\n",
        "]\n",
        "\n",
        "# Create new file - this will overwrite existing files.\n",
        "f = open('./insert.sql', 'w+')\n",
        "\n",
        "\n",
        "for category in categories:\n",
        "\n",
        "    # stream dataset\n",
        "    dataset = datasets.load_dataset(\n",
        "        \"McAuley-Lab/Amazon-Reviews-2023\",\n",
        "        f\"raw_meta_{category}\",\n",
        "        split=\"full\",\n",
        "        trust_remote_code=True,\n",
        "        streaming=True\n",
        "    )\n",
        "\n",
        "    # shuffle dataset and grab 100 items\n",
        "    # dataset = dataset.shuffle(seed=42, buffer_size=1_000)\n",
        "    dataset = dataset.take(NUM_RECORDS)\n",
        "\n",
        "    print(f\"Writing [{category}]...\\n\")\n",
        "\n",
        "    f.write(SQL_INSERT) # start with insert statement\n",
        "\n",
        "    for i, item in enumerate(dataset):\n",
        "        if all(x not in [item['title'], item['price'], item['main_category']] for x in ['None', None, '']):\n",
        "            line = f\"({(i%20) + 1}, $${unidecode(item['title'])}$$, {item['price']}, {LOCATION}, {POSTAL_CODE}, {STATUS}, '{item['main_category']}')\"\n",
        "\n",
        "            if i > 0:\n",
        "                f.write(',\\n' + line)\n",
        "            else:\n",
        "                f.write(line)\n",
        "\n",
        "    f.write(';\\n')\n",
        "\n",
        "    f.write('\\n\\n') # add new lines\n",
        "\n",
        "f.close()"
      ],
      "metadata": {
        "id": "Kg4SPfoGsETU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write `data.csv` - use this instead"
      ],
      "metadata": {
        "id": "zDauBZSYsKWq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4VDIveLd4eq",
        "outputId": "fb229936-0c93-4302-b6db-50dfda8f9278"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing [Appliances]...\n",
            "\n",
            "Writing [Automotive]...\n",
            "\n",
            "Writing [Beauty_and_Personal_Care]...\n",
            "\n",
            "Writing [Cell_Phones_and_Accessories]...\n",
            "\n",
            "Writing [Clothing_Shoes_and_Jewelry]...\n",
            "\n",
            "Writing [Electronics]...\n",
            "\n",
            "Writing [Health_and_Household]...\n",
            "\n",
            "Writing [Home_and_Kitchen]...\n",
            "\n",
            "Writing [Industrial_and_Scientific]...\n",
            "\n",
            "Writing [Musical_Instruments]...\n",
            "\n",
            "Writing [Office_Products]...\n",
            "\n",
            "Writing [Patio_Lawn_and_Garden]...\n",
            "\n",
            "Writing [Sports_and_Outdoors]...\n",
            "\n",
            "Writing [Tools_and_Home_Improvement]...\n",
            "\n",
            "Writing [Video_Games]...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from random import randrange\n",
        "\n",
        "# Number of records per category.\n",
        "NUM_RECORDS = 20_000\n",
        "\n",
        "# Constants\n",
        "# SQL_INSERT = \"INSERT INTO public.\\\"Listings\\\" (seller_id, title, price, location, postal_code, status, listed_at, last_updated_at, category) VALUES\\n\"\n",
        "FIELDS = ['seller_id', 'title', 'price', 'latitude', 'longitude', 'postal_code', 'status', 'listed_at', 'last_updated_at', 'category']\n",
        "LATITUDE = 48.378400\n",
        "LONGITUDE = -123.415600\n",
        "POSTAL_CODE = 'V8R6N2'\n",
        "STATUS = 'AVAILABLE'\n",
        "TIMESTAMP=datetime.datetime(2024, 7, 1)\n",
        "\n",
        "\n",
        "\n",
        "# from https://huggingface.co/datasets/McAuley-Lab/Amazon-Reviews-2023/tree/main/raw/meta_categories\n",
        "categories = [\n",
        "    'Appliances',\n",
        "    'Automotive',\n",
        "    'Beauty_and_Personal_Care',\n",
        "    'Cell_Phones_and_Accessories',\n",
        "    'Clothing_Shoes_and_Jewelry',\n",
        "    'Electronics',\n",
        "    'Health_and_Household',\n",
        "    'Home_and_Kitchen',\n",
        "    'Industrial_and_Scientific',\n",
        "    'Musical_Instruments',\n",
        "    'Office_Products',\n",
        "    'Patio_Lawn_and_Garden',\n",
        "    'Sports_and_Outdoors',\n",
        "    'Tools_and_Home_Improvement',\n",
        "    'Video_Games'\n",
        "]\n",
        "\n",
        "# Create csv file - this will overwrite existing files.\n",
        "csv_file = open('./listings.csv', 'w+')\n",
        "\n",
        "writer = csv.DictWriter(csv_file, fieldnames=FIELDS)\n",
        "writer.writeheader()\n",
        "\n",
        "for category in categories:\n",
        "\n",
        "    # stream dataset\n",
        "    dataset = datasets.load_dataset(\n",
        "        \"McAuley-Lab/Amazon-Reviews-2023\",\n",
        "        f\"raw_meta_{category}\",\n",
        "        split=\"full\",\n",
        "        trust_remote_code=True,\n",
        "        streaming=True\n",
        "    )\n",
        "\n",
        "    # shuffle dataset and grab 100 items\n",
        "    # dataset = dataset.shuffle(seed=42, buffer_size=1_000)\n",
        "    dataset = dataset.take(NUM_RECORDS)\n",
        "\n",
        "    print(f\"Writing [{category}]...\\n\")\n",
        "    for i, item in enumerate(dataset):\n",
        "        try:\n",
        "            float(item['price'])\n",
        "            if all(x not in [item['title'], item['price'], item['main_category']] for x in ['None', None, '']):\n",
        "                row = [{\n",
        "                    'seller_id': randrange(1, 4021),\n",
        "                    'title': unidecode(item['title']),\n",
        "                    'price': item['price'],\n",
        "                    'latitude': LATITUDE,\n",
        "                    'longitude': LONGITUDE,\n",
        "                    'postal_code': POSTAL_CODE,\n",
        "                    'status': STATUS,\n",
        "                    'listed_at': TIMESTAMP,\n",
        "                    'last_updated_at': TIMESTAMP,\n",
        "                    'category': item['main_category']\n",
        "                }]\n",
        "\n",
        "                writer.writerows(row)\n",
        "        except :\n",
        "            continue\n",
        "\n",
        "csv_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download file"
      ],
      "metadata": {
        "id": "hdMSywQG7lQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "# files.download('insert.sql')\n",
        "files.download('listings.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "SI_ksEzqndIu",
        "outputId": "ee67e15f-2c1e-4214-cfef-e823569ffdc2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_449a0dc9-7ef3-4436-bb6c-796f6f2c7d61\", \"listings.csv\", 34057070)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}